{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# House Prices in Argentina\n",
    "## Project Code and Descriptions\n",
    "### JonPaul Ferzacca & Joey Musholt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from geopy.distance import great_circle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Sets - ar_properties.csv & ars-usd.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "data = pd.read_csv('/Users/jp_ferzacca/Data_Mining/ar_properties.csv')\n",
    "ars_usd = pd.read_csv('/Users/jp_ferzacca/Data_Mining/CSCI-4502_Data_Mining_Project/ars-usd.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'ad_type', 'start_date', 'end_date', 'created_on', 'lat', 'lon',\n",
      "       'l1', 'l2', 'l3', 'l4', 'l5', 'l6', 'rooms', 'bedrooms', 'bathrooms',\n",
      "       'surface_total', 'surface_covered', 'price', 'currency', 'price_period',\n",
      "       'title', 'description', 'property_type', 'operation_type'],\n",
      "      dtype='object')\n",
      "                         id    ad_type  start_date    end_date  created_on  \\\n",
      "0  wdQ5hWhv8P14T7Sh9g4QCg==  Propiedad  2020-12-25  9999-12-31  2020-12-25   \n",
      "1  nnMBYZ4RMRY+vm753EtA+g==  Propiedad  2020-12-25  9999-12-31  2020-12-25   \n",
      "2  +dnVA1K6JxzL1zAjOEQ1pA==  Propiedad  2020-12-25  2020-12-29  2020-12-25   \n",
      "3  dLHXKN5/sRZpm9Yk0yI2nA==  Propiedad  2020-12-25  2020-12-29  2020-12-25   \n",
      "4  wtw/k887EPipd37UYHKb1Q==  Propiedad  2020-12-25  9999-12-31  2020-12-25   \n",
      "\n",
      "         lat        lon         l1                        l2        l3  ...  \\\n",
      "0 -32.716652 -68.642692  Argentina                   Mendoza       NaN  ...   \n",
      "1 -24.797723 -65.467514  Argentina                     Salta       NaN  ...   \n",
      "2 -34.919373 -58.020591  Argentina    Bs.As. G.B.A. Zona Sur  La Plata  ...   \n",
      "3 -34.919455 -58.024807  Argentina    Bs.As. G.B.A. Zona Sur  La Plata  ...   \n",
      "4 -34.364924 -58.783143  Argentina  Bs.As. G.B.A. Zona Norte   Escobar  ...   \n",
      "\n",
      "  bathrooms surface_total  surface_covered  price  currency  price_period  \\\n",
      "0       NaN         350.0            350.0    NaN       NaN           NaN   \n",
      "1       NaN        1541.0           1541.0    NaN       NaN       Mensual   \n",
      "2       NaN        1000.0           1000.0    NaN       NaN       Mensual   \n",
      "3       NaN        1000.0           1000.0    NaN       NaN       Mensual   \n",
      "4       NaN       18164.0          18164.0    NaN       NaN       Mensual   \n",
      "\n",
      "                                               title  \\\n",
      "0                     Excelentes Lotes Sobre Ruta 34   \n",
      "1  TERRENO + VENTA + JARDINES DE SAN LORENZO +150...   \n",
      "2               Lote en Venta de 1000 m2 en La Plata   \n",
      "3               Lote en Venta de 1000 m2 en La Plata   \n",
      "4                                 PANAMERICANA 47300   \n",
      "\n",
      "                                         description  property_type  \\\n",
      "0  Corredor Responsable: VICTOR E. MONTIVERO - C....           Lote   \n",
      "1  Corredor Responsable: Pablo Castañeda - C.U.C....           Lote   \n",
      "2  Corredor Responsable: Rico Sebastián - Martill...           Lote   \n",
      "3  Corredor Responsable: Rico Sebastián - Martill...           Lote   \n",
      "4  Nave principal 66 x 90 m:    6005 m2 cubiertos...           Otro   \n",
      "\n",
      "  operation_type  \n",
      "0          Venta  \n",
      "1          Venta  \n",
      "2          Venta  \n",
      "3          Venta  \n",
      "4          Venta  \n",
      "\n",
      "[5 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "# Initial Data Exploration\n",
    "print(data.columns)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                       0\n",
      "ad_type                  0\n",
      "start_date               0\n",
      "end_date                 0\n",
      "created_on               0\n",
      "lat                 150811\n",
      "lon                 151745\n",
      "l1                       0\n",
      "l2                       0\n",
      "l3                   53327\n",
      "l4                  760504\n",
      "l5                  994999\n",
      "l6                 1000000\n",
      "rooms               473577\n",
      "bedrooms            533787\n",
      "bathrooms           223267\n",
      "surface_total       620499\n",
      "surface_covered     618549\n",
      "price                36902\n",
      "currency             38281\n",
      "price_period        636985\n",
      "title                    0\n",
      "description             18\n",
      "property_type            0\n",
      "operation_type           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of missing values in each column\n",
    "missing_values = data.isna().sum()\n",
    "# Print the number of missing values for each column\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows missing both 'surface_total' and 'surface_covered': 569991\n"
     ]
    }
   ],
   "source": [
    "# Count rows missing both 'surface_total' and 'surface_covered'\n",
    "missing_surface = data[data['surface_total'].isna() & data['surface_covered'].isna()]\n",
    "num_missing_surface = len(missing_surface)\n",
    "print(f\"Number of rows missing both 'surface_total' and 'surface_covered': {num_missing_surface}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows missing both 'rooms' and 'bedrooms': 417303\n",
      "Number of rows missing 'rooms', 'bedrooms', and 'bathrooms': 173330\n"
     ]
    }
   ],
   "source": [
    "# Count rows missing both 'rooms' and 'bedrooms'\n",
    "missing_rooms = data[data['rooms'].isna() & data['bedrooms'].isna()]\n",
    "num_missing_rooms = len(missing_rooms)\n",
    "print(f\"Number of rows missing both 'rooms' and 'bedrooms': {num_missing_rooms}\")\n",
    "\n",
    "# Count rows missing 'rooms', 'bedrooms', and 'bathrooms'\n",
    "missing_rooms_bedrooms_bathrooms = data[data['rooms'].isna() & data['bedrooms'].isna() & data['bathrooms'].isna()]\n",
    "num_missing_rooms_bedrooms_bathrooms = len(missing_rooms_bedrooms_bathrooms)\n",
    "\n",
    "print(f\"Number of rows missing 'rooms', 'bedrooms', and 'bathrooms': {num_missing_rooms_bedrooms_bathrooms}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning and Preproccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of rows: 1000000\n",
      "Number of rows after removing missing/zero prices: 961581\n"
     ]
    }
   ],
   "source": [
    "# Remove rows where price is zero or NaN\n",
    "original_length = len(data)\n",
    "data = data[data['price'] > 0]\n",
    "cleaned_length = len(data)\n",
    "print(f\"Original number of rows: {original_length}\")\n",
    "print(f\"Number of rows after removing missing/zero prices: {cleaned_length}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['start_date', 'end_date', 'created_on', 'lat', 'lon', 'l2', 'l3',\n",
      "       'rooms', 'bedrooms', 'bathrooms', 'surface_total', 'surface_covered',\n",
      "       'price', 'property_type', 'operation_type'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Drop columns that are no longer needed or are identifiers\n",
    "columns_to_drop = ['id', 'ad_type', 'l1', 'l4', 'l5', 'l6', 'title', 'description', 'currency', 'price_period']\n",
    "\n",
    "# Check if each column in columns_to_drop exists in the DataFrame before dropping\n",
    "columns_to_drop = [col for col in columns_to_drop if col in data.columns]\n",
    "\n",
    "# Drop the columns\n",
    "data = data.drop(columns=columns_to_drop, axis=1)\n",
    "\n",
    "# Display the remaining columns to verify\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows where 'rooms', 'bedrooms', and 'bathrooms' are all missing\n",
    "data_cleaned = data.dropna(subset=['rooms', 'bedrooms', 'bathrooms'], how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of rows: 961581\n",
      "Number of rows after removal: 798510\n"
     ]
    }
   ],
   "source": [
    "# Display the shape of the new DataFrame to verify the rows were removed\n",
    "print(f\"Original number of rows: {len(data)}\")\n",
    "print(f\"Number of rows after removal: {len(data_cleaned)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing column: start_date\n",
      "Processing column: end_date\n",
      "Processing column: created_on\n"
     ]
    }
   ],
   "source": [
    "# Function to filter valid dates\n",
    "def filter_valid_dates(data, column):\n",
    "    print(f\"Processing column: {column}\")\n",
    "    if column not in data.columns:\n",
    "        print(f\"Column {column} not found in DataFrame.\")\n",
    "        return data\n",
    "    data[column] = pd.to_datetime(data[column], errors='coerce')\n",
    "    return data.dropna(subset=[column])\n",
    "\n",
    "# Apply the function to date columns\n",
    "data = filter_valid_dates(data, 'start_date')\n",
    "data = filter_valid_dates(data, 'end_date')\n",
    "data = filter_valid_dates(data, 'created_on')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert latitude and longitude to float\n",
    "data.lat = data.lat.astype(float)\n",
    "data.lon = data.lon.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to reformat date strings to datetime objects\n",
    "def reformat_dates(date_array):\n",
    "    dates_dict = {}\n",
    "    formatted_date = []\n",
    "    for date in date_array:\n",
    "        if date not in dates_dict:\n",
    "            try:\n",
    "                # Try converting the date, and use a placeholder if it fails\n",
    "                dates_dict[date] = pd.to_datetime(date)\n",
    "            except (pd._libs.tslibs.np_datetime.OutOfBoundsDatetime, OverflowError):\n",
    "                # Use a placeholder date such as '3/15/2021' or set to NaN\n",
    "                dates_dict[date] = pd.to_datetime('3/15/2021')  # or pd.NaT for missing time\n",
    "        formatted_date.append(dates_dict[date])\n",
    "    return formatted_date\n",
    "\n",
    "# Apply the function to date columns\n",
    "data.start_date = reformat_dates(data.start_date)\n",
    "data.end_date = reformat_dates(data.end_date)\n",
    "data.created_on = reformat_dates(data.created_on)\n",
    "ars_usd.date = reformat_dates(ars_usd.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numeric columns from strings to floats\n",
    "numeric_features = ['rooms', 'bedrooms', 'bathrooms', 'surface_total', 'surface_covered', 'price']\n",
    "data[numeric_features] = data[numeric_features].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Transformations\n",
    "\n",
    "# Fill NaN values in numeric features and ensure no negative values\n",
    "for feature in numeric_features:\n",
    "    data[feature] = data[feature].fillna(0).clip(lower=0)\n",
    "\n",
    "# Apply log transformation\n",
    "def apply_log_transform(data, features):\n",
    "    for feature in features:\n",
    "        data['log_' + feature] = np.log1p(data[feature])  # log1p is used to handle zero values\n",
    "    return data\n",
    "\n",
    "data = apply_log_transform(data, numeric_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create date-related features\n",
    "data['listing_duration'] = (data['end_date'] - data['start_date']).dt.days  # Duration in days\n",
    "data['listing_age'] = (pd.Timestamp('today') - data['created_on']).dt.days  # Age in days from today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding of categorical variables\n",
    "categorical_features = ['property_type', 'operation_type']\n",
    "data = pd.get_dummies(data, columns=categorical_features)\n",
    "\n",
    "additional_categorical_features = ['l2', 'l3', ...]  # Add other categorical columns here\n",
    "for feature in additional_categorical_features:\n",
    "    if feature in data.columns:\n",
    "        data = pd.get_dummies(data, columns=[feature])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate distance to a central point in Buenos Aires\n",
    "def calculate_distance(lat1, lon1, lat2, lon2):\n",
    "    if pd.isna(lat1) or pd.isna(lon1):\n",
    "        return np.nan  # or return some default value\n",
    "    return great_circle((lat1, lon1), (lat2, lon2)).kilometers\n",
    "\n",
    "central_point = (-34.6037, -58.3816)  # Coordinates of Buenos Aires central\n",
    "data['distance_to_central'] = data.apply(lambda row: calculate_distance(row['lat'], row['lon'], central_point[0], central_point[1]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert datetime columns to numeric\n",
    "reference_date = pd.Timestamp('2023-01-01')  # Choose an appropriate reference date\n",
    "data['days_since_start'] = (data['start_date'] - reference_date).dt.days\n",
    "data['days_since_end'] = (data['end_date'] - reference_date).dt.days\n",
    "data['days_since_created'] = (data['created_on'] - reference_date).dt.days\n",
    "\n",
    "# Drop the original datetime columns\n",
    "data = data.drop(['start_date', 'end_date', 'created_on'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate numeric and categorical columns\n",
    "numeric_cols = data.select_dtypes(include=[np.number]).columns\n",
    "categorical_cols = data.select_dtypes(exclude=[np.number]).columns\n",
    "\n",
    "# Remove 'price' and 'log_price' from numeric_cols if present\n",
    "numeric_cols = [col for col in numeric_cols if col not in ['price', 'log_price']]\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X = data.drop(['price', 'log_price'], axis=1)  # Drop target variable and non-feature columns\n",
    "y = data['log_price']  # Assuming log_price is the target variable\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation of missing values for numeric data\n",
    "imputer_numeric = SimpleImputer(strategy='median')\n",
    "X_train_numeric = imputer_numeric.fit_transform(X_train[numeric_cols])\n",
    "X_test_numeric = imputer_numeric.transform(X_test[numeric_cols])\n",
    "\n",
    "# Convert boolean columns to integers\n",
    "for col in X_train.columns:\n",
    "    if X_train[col].dtype == 'bool':\n",
    "        X_train[col] = X_train[col].astype(int)\n",
    "        X_test[col] = X_test[col].astype(int)\n",
    "        \n",
    "# For categorical data, you can use a different strategy, like 'most_frequent' or 'constant'\n",
    "imputer_categorical = SimpleImputer(strategy='most_frequent')\n",
    "X_train_categorical = imputer_categorical.fit_transform(X_train[categorical_cols])\n",
    "X_test_categorical = imputer_categorical.transform(X_test[categorical_cols])\n",
    "\n",
    "# Combine numeric and categorical data back into one DataFrame\n",
    "X_train_imputed = np.concatenate((X_train_numeric, X_train_categorical), axis=1)\n",
    "X_test_imputed = np.concatenate((X_test_numeric, X_test_categorical), axis=1)\n",
    "\n",
    "# Fitting the Linear Regression Model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_imputed, y_train)\n",
    "\n",
    "# Predicting on the test set\n",
    "y_pred = model.predict(X_test_imputed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 0.676857385374826\n",
      "Mean Squared Error (MSE): 1.0080877763481029\n",
      "R-squared (R²): 0.5062553440335252\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"R-squared (R²): {r2}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
